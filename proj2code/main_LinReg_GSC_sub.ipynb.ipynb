{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "#import clustering from sklearn.cluster using KMeans\n",
    "from sklearn.utils import shuffle \n",
    "#import shuffle utility from sklearn utilities\n",
    "import numpy as np \n",
    "#import numpy library as np\n",
    "import csv \n",
    "#import csv import and export format for spreadsheets and databases\n",
    "import math \n",
    "#import math library \n",
    "import matplotlib.pyplot \n",
    "#import pythons math plot library\n",
    "from matplotlib import pyplot as plt \n",
    "#import plot from math plot library \n",
    "import pandas as pd \n",
    "#imports pandas methods as pd since python built-in methods can overlap panda methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing of Human Observed Dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e83d3d861ef7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;31m#print(get_features_gsc)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0mget_features_gsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'img_id_A'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'img_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m \u001b[0mget_features_gsc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_features_gsc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataFrame_features_gsc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'img_id_B'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mright_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'img_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[0mget_features_gsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'img_id_B'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'img_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     60\u001b[0m                          \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m                          validate=validate)\n\u001b[1;32m---> 62\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\reshape\\merge.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    580\u001b[0m             \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mldata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m             \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mllabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjoin_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m             concat_axis=0, copy=self.copy)\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \u001b[0mtyp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5419\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5420\u001b[0m             b = make_block(\n\u001b[1;32m-> 5421\u001b[1;33m                 \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5422\u001b[0m                 placement=placement)\n\u001b[0;32m   5423\u001b[0m         \u001b[0mblocks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   5563\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[0;32m   5564\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[1;32m-> 5565\u001b[1;33m                  for ju in join_units]\n\u001b[0m\u001b[0;32m   5566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   5563\u001b[0m     to_concat = [ju.get_reindexed_values(empty_dtype=empty_dtype,\n\u001b[0;32m   5564\u001b[0m                                          upcasted_na=upcasted_na)\n\u001b[1;32m-> 5565\u001b[1;33m                  for ju in join_units]\n\u001b[0m\u001b[0;32m   5566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5567\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\internals.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m   5873\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5874\u001b[0m                 values = algos.take_nd(values, indexer, axis=ax,\n\u001b[1;32m-> 5875\u001b[1;33m                                        fill_value=fill_value)\n\u001b[0m\u001b[0;32m   5876\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5877\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans #import clustering from sklearn.cluster using KMeans\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np #import numpy library as np\n",
    "import csv #import csv import and export format for spreadsheets and databases\n",
    "import math #import math library \n",
    "import matplotlib.pyplot #import pythons math plot library\n",
    "from matplotlib import pyplot as plt #import plot from math plot library \n",
    "import pandas as pd #imports pandas methods as pd since python built-in methods can overlap panda methods.\n",
    "\n",
    "\n",
    "dataFrame_same_gsc = pd.read_csv('same_pairs_gsc.csv')\n",
    "#print(dataFrame_same)\n",
    "dataFrame_diff_gsc = pd.read_csv('diffn_pairs_gsc.csv')\n",
    "#print(dataFrame_diff)\n",
    "dataFrame_diff_random_gsc = dataFrame_diff_gsc.sample(n=len(dataFrame_same_gsc))\n",
    "#print(dataFrame_diff_random)\n",
    "same_diff_one_gsc = dataFrame_same_gsc.append(dataFrame_diff_random_gsc)\n",
    "#print(same_diff_one)\n",
    "same_diff_shuffle_gsc = shuffle(same_diff_one_gsc)\n",
    "#print(same_diff_shuffle) \n",
    "dataFrame_features_gsc = pd.read_csv('GSC-Features.csv')\n",
    "#print(dataFrame_human_features)\n",
    "\n",
    "get_features_gsc = pd.merge(same_diff_shuffle_gsc,dataFrame_features_gsc, how='left', left_on='img_id_A', right_on='img_id')\n",
    "#print(get_features_gsc)\n",
    "get_features_gsc.drop(columns = ['img_id_A', 'img_id'], axis=1,inplace=True)\n",
    "get_features_gsc=pd.merge(get_features_gsc,dataFrame_features_gsc,how='left',left_on='img_id_B',right_on='img_id')\n",
    "get_features_gsc.drop(columns = ['img_id_B', 'img_id'], axis=1,inplace=True)\n",
    "\n",
    "#features_sub = pd.DataFrame(columns=['f1_x', 'f2_x', 'f3_x', 'f4_x', 'f5_x', 'f6_x', 'f7_x', 'f8_x', 'f9_x'])\n",
    "\n",
    "features_final_gsc=get_features_gsc.loc[:,:]\n",
    "feature_target_gsc=get_features_gsc.loc[:,\"target\"]\n",
    "print(feature_target_gsc)\n",
    "\n",
    "get_features_new_gsc =get_features_gsc\n",
    "\n",
    "get_features_new_gsc.drop(columns = ['target'], axis=1,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "for i in range(0,512):\n",
    "    get_features_new_gsc['f'+ str(i+1) +'_x'] = abs(get_features_new_gsc['f'+ str(i+1) +'_x'] - get_features_new_gsc['f'+ str(i+1) +'_y'])\n",
    "\n",
    "#print(get_hu_features)\n",
    "\n",
    "get_features_gsc_new = get_features_new_gsc.iloc[:,0:512]\n",
    "print(get_features_gsc_new)\n",
    "#print(features_final) # display the concatenated output\n",
    "#print(feature_target) #display the target output\n",
    "\n",
    "\n",
    "features_final_gsc.to_csv('New_Features__gsc_sub.csv', index=False, header=False) \n",
    "# write the concatenated output to csv file\n",
    "feature_target_gsc.to_csv('New_Target__gsc_sub.csv', index=False, header=False)\n",
    "# write the target output to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features_final_gsc.drop(columns = ['target'], axis=1,inplace=True)\n",
    "#features_final_gsc.to_csv('New_Features_gsc.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_target_gsc.to_csv('New_Target_gsc.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Different Parameters & Functions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0.0 \n",
    "#initialize Max Accurary = 0.0 \n",
    "\n",
    "maxIter = 0 \n",
    "#initialize Max Iterations = 0\n",
    "\n",
    "C_Lambda = 2  \n",
    "#initialize Lambda value\n",
    "\n",
    "TrainingPercent = 80 \n",
    "#initialize Training Data Set % = 80\n",
    "\n",
    "ValidationPercent = 10 \n",
    "#initialize Validation Data Set % = 10\n",
    "\n",
    "TestPercent = 10 \n",
    "#initialize Test Data Set % = 10 \n",
    "\n",
    "M = 10 \n",
    "#initialize number of M Basis Functions = 10\n",
    "\n",
    "PHI = [] \n",
    "#initialize PHI as an Array \n",
    "\n",
    "IsSynthetic = False \n",
    "#set IsSynthetic value as False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation for Linear Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTargetVector(filePath):                         #define GetTarget Vector code block with source filePath\n",
    "    t = []                                             #define target vector array\n",
    "    with open(filePath, 'rU') as f:                    #open or set directory for the source filePath as f\n",
    "        reader = csv.reader(f)                         #open filePath defined as f in csv reader \n",
    "        for row in reader:                             #from the filepath access rows \n",
    "            t.append(int(row[0]))                      #append row values to target array\n",
    "    return t                                           #return the values to target dataset t\n",
    "\n",
    "def GenerateRawData(filePath, IsSynthetic): \n",
    "                                 #define GenerateRawData with source filePath and IsSynthetic value set as False    \n",
    "    dataMatrix = []              #define dataMatrix as an array to store values\n",
    "    with open(filePath, 'rU') as fi: #open or set directory for the source filePath as fi\n",
    "        reader = csv.reader(fi)      #open filePath defined as fi in csv reader\n",
    "        for row in reader:           #from the filepath access rows\n",
    "            dataRow = []             #dataRow array saves the values\n",
    "            for column in row: #for every column in datarow\n",
    "                dataRow.append(float(column)) #append the value of dataRow as float\n",
    "            dataMatrix.append(dataRow)        #append the dataRow matrix in dataMatrix   \n",
    "    \n",
    "    if IsSynthetic == False : \n",
    "        dataMatrix = np.delete(dataMatrix, [5,6,7,8,9], axis=1) # delete array elements from dataMatrix \n",
    "    dataMatrix = np.transpose(dataMatrix)                       # take the transpose of the dataMatrix     \n",
    "    return dataMatrix                                  #return the dataMatrix which was transposed in above line\n",
    "\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80): \n",
    "                                                    #define GenerateTrainingTarget data with data and percentage\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01))) \n",
    "                                                   #covert to float and put the values in TrainingLen\n",
    "    t           = rawTraining[:TrainingLen]        #put the values of rawTraining set into target vector array t\n",
    "                                                 \n",
    "    return t                                       #return new target vector array t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80): \n",
    "                                                  #define GenerateTrainingDataMatrix with rawData and percentage\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "                                                  #covert to float and put the values in T_Len\n",
    "    d2 = rawData[:,0:T_len]                       #put values in d2 \n",
    "    return d2                                     #return d2 with changes\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "                                              #define GenerateValData with rawData, TrainingCount and ValPercent \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01)) #calculate val_size with float values\n",
    "    V_End = TrainingCount + valSize \n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]             #update the dataMatrix with new values \n",
    "    return dataMatrix                                         #return the updated dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "                                                               #define GenerateValTargetVector for taget vectors \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))     #calculate val_size with float values\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]                          #update the target Vector values with new values\n",
    "    return t                                                   #return the updated array t\n",
    "\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic): \n",
    "                                                               #define GenerateBigSigma for traning Vectors\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))              #BigSigma takes the values of the Data\n",
    "    DataT       = np.transpose(Data)                           #take the transpose of the Data Matrix \n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01)) #calculate Training Lenth with float values       \n",
    "    varVect     = [] \n",
    "    for i in range(0,len(DataT[0])):                           #iterate the values for tranpose data range\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):                    #iterate the values for TrainingLen range\n",
    "            vct.append(Data[i][j])                             #append the values in array vct \n",
    "        varVect.append(np.var(vct))                            #append the values in array varVect\n",
    "    \n",
    "    for j in range(len(Data)):                                 #iterate the values for data range\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:                                    #if value True\n",
    "        BigSigma = np.dot(3,BigSigma)                          #calculate BigSigma\n",
    "    else: #if value False\n",
    "        BigSigma = np.dot(200,BigSigma)                        #calculate BigSigma\n",
    "        Add_Noise_BigSigma = BigSigma+0.00001*np.random.rand(1024,1024) #Add Noise to Big Sigma\n",
    "    return Add_Noise_BigSigma                                           #return the updated value of the BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):                       #define the GetScalar\n",
    "    R = np.subtract(DataRow,MuRow)                             #calculate R\n",
    "    T = np.dot(BigSigInv,np.transpose(R))                      #calculate T   \n",
    "    L = np.dot(R,T)                                            #calculate L\n",
    "    return L                                                   #return the updated value of L \n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):               #define the RadialBasis function    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv)) \n",
    "    return phi_x                                               #return the update value of phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80): #define the Phi Matrix \n",
    "    DataT = np.transpose(Data)                                    #take the transpose of DataT matrix\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))    #calculate TraningLen with float values         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))              #take the required PHI values\n",
    "    BigSigInv = np.linalg.inv(BigSigma)                           #calculate the BigSigma inverse value\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv) #calculate the PHI values\n",
    "    return PHI                                                              #return updated PHI values\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):                                    #define the GetWeightsCLosedForm\n",
    "    Lambda_I = np.identity(len(PHI[0]))                                      #Lamda_I = identity matrix of PHI\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)                                          #calculate phi transpose\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)                                          #calculate phiT.phi\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)                                   #calculate lambda + phiT.phi\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)                                  #calculate phi inverse\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)              #calculate dot product of phi inverse and transpose\n",
    "    W           = np.dot(INTER, T)                                           #calculate w\n",
    "    return W                                                                 #return the updated value of w\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):  #define the GetPhiMatrix\n",
    "    DataT = np.transpose(Data)                                     #transpose of DataT\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))     #calculate TraningLen with float values         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))               #Phi matrix  \n",
    "    BigSigInv = np.linalg.inv(BigSigma)                            #calculate the BigSigma Inverse\n",
    "    for  C in range(0,len(MuMatrix)): \n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv) \n",
    "                                                                      #for each range of Mu Matrix calculate phi\n",
    "    return PHI                                                        #return the updated values of Phi\n",
    "\n",
    "def GetValTest(VAL_PHI,W):                                            #define GetValTest for validation\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))                               #calculate W . ValPhi\n",
    "    return Y                                                          #return the update values of Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):                                 #define the GetERMS block\n",
    "    sum = 0.0                                                         #initialize sum = 0.0\n",
    "    t=0                                                               #initialize t = 0 for target vectors\n",
    "    accuracy = 0.0                                                    #initialize accuracy = 0.0\n",
    "    counter = 0                                                       #initialize counter to 0\n",
    "    val = 0.0                                                         #initialize val to 0\n",
    "    for i in range (0,len(VAL_TEST_OUT)): \n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)     #calculate the sum\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1                                                #increment the counter\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))        #calculate the accuracy of the data\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT)))) # return the value of ERMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEERAJ\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: 'U' mode is deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\NEERAJ\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: 'U' mode is deprecated\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "RawTarget = GetTargetVector('New_Target_gsc.csv')              #feed the machine the target ouput csv file\n",
    "RawData   = GenerateRawData('New_Features_gsc.csv',IsSynthetic)#feed the machine the concatenated output csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(114450,)\n",
      "(1019, 114450)\n"
     ]
    }
   ],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget,TrainingPercent)) #give traning target data\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)         #give training data from matrix\n",
    "\n",
    "print(TrainingTarget.shape) \n",
    "print(TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14306,)\n",
      "(1019, 14306)\n"
     ]
    }
   ],
   "source": [
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget)))) \n",
    "ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget))) \n",
    "#give the validation data with generated data\n",
    "\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14306,)\n",
      "(1019, 14306)\n"
     ]
    }
   ],
   "source": [
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct))) #give test data \n",
    "\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form Solution [Finding Weights using Moore- Penrose pseudo- Inverse Matrix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []                  \n",
    "#define the Erms array\n",
    "\n",
    "AccuracyArr = []              \n",
    "#define accuracy array\n",
    "\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData)) \n",
    "#calculate the clustering using KMeans\n",
    "\n",
    "Mu = kmeans.cluster_centers_  \n",
    "#calculate the Mu value\n",
    "\n",
    "BigSigma     = GenerateBigSigma(RawData, Mu, TrainingPercent,IsSynthetic)   \n",
    "#calculate the BigSigma value\n",
    "\n",
    "TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)         \n",
    "#calculate the traning phi value\n",
    "\n",
    "W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda)) \n",
    "#calculate the regularizer value\n",
    "\n",
    "TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "#get the Test phi value\n",
    "\n",
    "VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100) \n",
    "#get the val phi value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Mu.shape)\n",
    "print(BigSigma.shape)\n",
    "print(TRAINING_PHI.shape)\n",
    "print(W.shape)\n",
    "print(VAL_PHI.shape)\n",
    "print(TEST_PHI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Erms on Training, Validation and Test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_TEST_OUT  = GetValTest(TRAINING_PHI,W) \n",
    "VAL_TEST_OUT = GetValTest(VAL_PHI,W)\n",
    "TEST_OUT     = GetValTest(TEST_PHI,W)\n",
    "\n",
    "TrainingAccuracy   = str(GetErms(TR_TEST_OUT,TrainingTarget)) #get erms of training data accuracy\n",
    "ValidationAccuracy = str(GetErms(VAL_TEST_OUT,ValDataAct))    #get erms of validation data accuracy\n",
    "TestAccuracy       = str(GetErms(TEST_OUT,TestDataAct))       #get erms of test data accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('UBITname      = nabhyank')\n",
    "print ('Person Number = 50290958')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"------------------LeToR Data------------------------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Closed Form with Radial Basis Function-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"M = \" + str (M))\n",
    "print (\"Lambda = \" + str(C_Lambda))\n",
    "print (\"E_rms Training   = \" + str(float(TrainingAccuracy.split(',')[1])))\n",
    "print (\"Traning Accuracy   = \" + str(float(TrainingAccuracy.split(',')[0])))\n",
    "print (\"E_rms Validation = \" + str(float(ValidationAccuracy.split(',')[1])))\n",
    "print (\"Validation Accuracy   = \" + str(float(ValidationAccuracy.split(',')[0])))\n",
    "print (\"E_rms Testing    = \" + str(float(TestAccuracy.split(',')[1])))\n",
    "print (\"Testing Accuracy   = \" + str(float(TestAccuracy.split(',')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent solution for Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('----------------------------------------------------')\n",
    "print ('--------------Please Wait for 2 mins!----------------')\n",
    "print ('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now        = np.dot(220, W)                                        #take the dot product to calculate W now \n",
    "La           = 6                                                     #initialize Lambda value\n",
    "learningRate = 0.01                                                  #set the learning rate value\n",
    "L_Erms_Val   = []                                                    #define Erms value of validation data array\n",
    "L_Erms_Acc_Val   = []                                                #define Erms value of accuracy data array\n",
    "L_Erms_TR    = []                                                    #define Erms value of training data array\n",
    "L_Erms_Acc_TR   = []                                                 #define Erms value of accuracy data array\n",
    "L_Erms_Test  = []                                                    #define Erms value of testing data array\n",
    "L_Erms_Acc_Test   = []                                               #define Erms value of accuracy data array\n",
    "W_Mat        = []                                                    #define W\n",
    "\n",
    "for i in range(0,400):                                               #iterate for range of 40 \n",
    "    \n",
    "    #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - np.dot(np.transpose(W_Now),TRAINING_PHI[i])),TRAINING_PHI[i]) \n",
    "    #calculate change in E_D\n",
    "    La_Delta_E_W  = np.dot(La,W_Now) \n",
    "    #calculate change in E_W dot product with lambda\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W) \n",
    "    #calculate change in E    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E) \n",
    "    #change in W is negative of dot product of learning rate and change in E  \n",
    "    W_T_Next      = W_Now + Delta_W \n",
    "    #next W = current w + change in W\n",
    "    W_Now         = W_T_Next \n",
    "    #update current W to next W where W is the weight\n",
    "    \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(TRAINING_PHI,W_T_Next) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget) \n",
    "    #calculate the erms training accuracy for all weights and phi values\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1])) \n",
    "    #append the values acquired in above calculation\n",
    "    L_Erms_Acc_TR.append(float(Erms_TR.split(',')[0]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(VAL_PHI,W_T_Next) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct) \n",
    "    #calculate the erms validation accuracy for all values of testing set and actual data values\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1])) \n",
    "    #append the values acquired in above calculation\n",
    "    L_Erms_Acc_Val.append(float(Erms_Val.split(',')[0]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(TEST_PHI,W_T_Next) \n",
    "    Erms_Test = GetErms(TEST_OUT,TestDataAct) \n",
    "    #calculate the erms testing accuracy \n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1])) \n",
    "    #append the values from above calculation\n",
    "    L_Erms_Acc_Test.append(float(Erms_Test.split(',')[0])) \n",
    "    #append the values from above calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('----------Gradient Descent Solution--------------------')\n",
    "print (\"M = \"+ str (M))\n",
    "print (\"La = \"+ str(La))\n",
    "print (\"Learning Rate = \"+ str(learningRate))\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "print (\"Traning Accuracy   = \" + str(np.around(max(L_Erms_Acc_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "print (\"Validation Accuracy   = \" + str(np.around(max(L_Erms_Acc_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "print (\"Testing Accuracy   = \" + str(np.around(max(L_Erms_Acc_TR),5)))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(L_Erms_TR)\n",
    "plt.title('Training ERMS')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(L_Erms_Acc_TR)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(L_Erms_Val)\n",
    "plt.title('Validation ERMS')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(L_Erms_Acc_Val)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.figure(3)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(L_Erms_Test)\n",
    "plt.title('Testing ERMS')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(L_Erms_Acc_Test)\n",
    "plt.title('Testing Accuracy')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
