{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans \n",
    "#import clustering from sklearn.cluster using KMeans\n",
    "from sklearn.utils import shuffle \n",
    "#import shuffle utility from sklearn utilities\n",
    "import numpy as np \n",
    "#import numpy library as np\n",
    "import csv \n",
    "#import csv import and export format for spreadsheets and databases\n",
    "import math \n",
    "#import math library \n",
    "import matplotlib.pyplot \n",
    "#import pythons math plot library\n",
    "from matplotlib import pyplot as plt \n",
    "#import plot from math plot library \n",
    "import pandas as pd \n",
    "#imports pandas methods as pd since python built-in methods can overlap panda methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing of Human Observed Dataset : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        target  f1_x  f2_x  f3_x  f4_x  f5_x  f6_x  f7_x  f8_x  f9_x   ...    \\\n",
      "0            1     0     0     0     0     0     0     0     0     0   ...     \n",
      "1            0     1     1     0     0     0     0     0     0     0   ...     \n",
      "2            0     1     1     0     0     0     0     0     0     0   ...     \n",
      "3            1     0     0     0     0     0     0     0     0     0   ...     \n",
      "4            0     0     0     0     0     0     0     0     0     0   ...     \n",
      "5            1     0     1     0     0     0     0     0     0     0   ...     \n",
      "6            1     0     1     0     0     0     0     0     0     0   ...     \n",
      "7            1     0     0     1     0     0     0     0     0     0   ...     \n",
      "8            1     0     0     1     0     1     0     0     0     0   ...     \n",
      "9            0     0     0     0     0     0     0     0     0     0   ...     \n",
      "10           0     1     0     0     0     0     0     1     0     0   ...     \n",
      "11           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "12           0     0     1     1     0     0     0     0     0     0   ...     \n",
      "13           0     1     0     0     0     0     0     0     0     0   ...     \n",
      "14           1     0     0     0     0     0     0     0     0     0   ...     \n",
      "15           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "16           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "17           1     0     0     0     0     0     0     0     0     0   ...     \n",
      "18           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "19           0     0     1     0     0     0     0     0     0     0   ...     \n",
      "20           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "21           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "22           1     0     0     1     0     0     0     0     0     0   ...     \n",
      "23           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "24           1     0     1     0     0     0     0     0     0     0   ...     \n",
      "25           1     1     1     0     0     0     0     1     1     0   ...     \n",
      "26           1     0     0     0     0     0     0     0     0     0   ...     \n",
      "27           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "28           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "29           0     0     0     0     0     0     0     0     0     0   ...     \n",
      "...        ...   ...   ...   ...   ...   ...   ...   ...   ...   ...   ...     \n",
      "143032       0     0     1     1     0     1     0     0     0     0   ...     \n",
      "143033       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143034       0     1     1     0     0     0     0     0     0     0   ...     \n",
      "143035       0     0     1     1     0     0     0     0     1     0   ...     \n",
      "143036       1     0     1     1     0     0     0     0     0     1   ...     \n",
      "143037       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143038       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143039       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143040       1     0     0     0     0     0     0     0     0     0   ...     \n",
      "143041       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143042       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143043       1     0     1     1     1     0     0     0     0     0   ...     \n",
      "143044       0     0     0     1     0     0     0     0     0     1   ...     \n",
      "143045       1     0     0     0     0     0     0     0     0     0   ...     \n",
      "143046       1     0     1     0     0     0     0     0     0     0   ...     \n",
      "143047       1     0     0     0     0     0     0     0     0     0   ...     \n",
      "143048       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143049       1     0     1     0     0     0     0     0     0     0   ...     \n",
      "143050       1     0     0     0     0     0     0     0     0     0   ...     \n",
      "143051       0     0     1     0     0     0     0     0     0     0   ...     \n",
      "143052       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143053       1     0     0     0     0     0     0     0     0     0   ...     \n",
      "143054       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143055       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143056       0     0     1     1     0     0     0     0     0     1   ...     \n",
      "143057       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143058       1     0     0     1     0     0     0     0     0     0   ...     \n",
      "143059       0     0     0     0     0     0     0     0     0     0   ...     \n",
      "143060       1     0     0     0     0     0     0     0     0     0   ...     \n",
      "143061       1     0     1     0     0     0     0     0     1     0   ...     \n",
      "\n",
      "        f503_y  f504_y  f505_y  f506_y  f507_y  f508_y  f509_y  f510_y  \\\n",
      "0            0       0       0       0       0       0       0       0   \n",
      "1            0       0       0       0       0       0       0       0   \n",
      "2            0       0       0       0       0       0       0       0   \n",
      "3            0       0       0       0       0       0       0       0   \n",
      "4            0       0       0       0       0       0       0       0   \n",
      "5            0       0       0       0       0       0       0       0   \n",
      "6            0       0       0       0       0       0       0       0   \n",
      "7            0       0       0       0       0       0       0       0   \n",
      "8            0       0       0       0       0       0       0       0   \n",
      "9            0       0       0       0       0       0       0       0   \n",
      "10           0       0       0       0       0       0       0       0   \n",
      "11           0       0       0       0       0       0       0       0   \n",
      "12           0       0       0       0       0       0       0       0   \n",
      "13           0       0       0       0       0       0       0       0   \n",
      "14           0       0       0       0       0       0       0       0   \n",
      "15           0       0       0       0       0       0       0       0   \n",
      "16           0       0       0       0       0       0       0       0   \n",
      "17           0       0       0       0       0       0       0       0   \n",
      "18           0       0       0       0       0       0       0       0   \n",
      "19           0       0       0       0       0       0       0       0   \n",
      "20           0       0       0       0       0       0       0       0   \n",
      "21           0       0       0       0       0       0       0       0   \n",
      "22           0       0       0       0       0       0       0       0   \n",
      "23           0       0       0       0       0       0       0       0   \n",
      "24           0       0       0       0       0       0       0       0   \n",
      "25           0       0       0       0       0       0       0       0   \n",
      "26           0       0       0       0       0       0       0       0   \n",
      "27           0       0       0       0       0       0       0       0   \n",
      "28           0       0       0       0       0       0       0       0   \n",
      "29           0       0       0       0       0       0       0       0   \n",
      "...        ...     ...     ...     ...     ...     ...     ...     ...   \n",
      "143032       0       0       0       0       0       0       0       0   \n",
      "143033       0       0       0       0       0       0       0       0   \n",
      "143034       0       0       0       0       0       0       0       0   \n",
      "143035       0       0       0       0       0       0       0       0   \n",
      "143036       0       0       0       0       0       0       0       0   \n",
      "143037       0       0       0       0       0       0       0       0   \n",
      "143038       0       0       0       0       0       0       0       0   \n",
      "143039       0       0       0       0       0       0       0       0   \n",
      "143040       0       0       0       0       0       0       0       0   \n",
      "143041       0       0       0       0       0       0       0       0   \n",
      "143042       0       0       0       0       0       0       0       0   \n",
      "143043       0       0       0       0       0       0       0       0   \n",
      "143044       0       0       0       0       0       0       0       0   \n",
      "143045       0       1       0       0       0       0       0       0   \n",
      "143046       0       0       0       0       0       0       0       0   \n",
      "143047       0       0       0       0       0       0       0       0   \n",
      "143048       0       0       0       0       0       0       0       0   \n",
      "143049       0       0       0       0       0       0       0       0   \n",
      "143050       0       0       0       0       0       0       0       0   \n",
      "143051       0       0       0       0       0       0       0       0   \n",
      "143052       0       0       0       0       0       0       0       0   \n",
      "143053       0       0       0       0       0       0       0       0   \n",
      "143054       0       0       0       0       0       0       0       0   \n",
      "143055       0       0       0       0       0       0       0       0   \n",
      "143056       0       0       0       0       0       0       0       0   \n",
      "143057       0       0       0       0       0       0       0       0   \n",
      "143058       0       0       0       0       0       0       0       0   \n",
      "143059       0       0       0       0       0       0       0       0   \n",
      "143060       0       0       0       0       0       0       0       0   \n",
      "143061       0       0       0       0       0       0       0       0   \n",
      "\n",
      "        f511_y  f512_y  \n",
      "0            0       0  \n",
      "1            0       0  \n",
      "2            0       0  \n",
      "3            0       0  \n",
      "4            0       0  \n",
      "5            0       0  \n",
      "6            0       0  \n",
      "7            0       0  \n",
      "8            0       0  \n",
      "9            0       0  \n",
      "10           0       0  \n",
      "11           0       0  \n",
      "12           0       0  \n",
      "13           0       0  \n",
      "14           0       0  \n",
      "15           0       0  \n",
      "16           0       0  \n",
      "17           0       0  \n",
      "18           0       0  \n",
      "19           0       0  \n",
      "20           0       0  \n",
      "21           0       0  \n",
      "22           0       0  \n",
      "23           0       0  \n",
      "24           0       0  \n",
      "25           0       0  \n",
      "26           0       0  \n",
      "27           0       0  \n",
      "28           0       0  \n",
      "29           0       0  \n",
      "...        ...     ...  \n",
      "143032       0       0  \n",
      "143033       0       0  \n",
      "143034       0       0  \n",
      "143035       0       0  \n",
      "143036       0       0  \n",
      "143037       0       0  \n",
      "143038       0       0  \n",
      "143039       0       0  \n",
      "143040       0       0  \n",
      "143041       0       0  \n",
      "143042       0       0  \n",
      "143043       0       0  \n",
      "143044       0       0  \n",
      "143045       0       0  \n",
      "143046       0       0  \n",
      "143047       0       0  \n",
      "143048       0       0  \n",
      "143049       0       0  \n",
      "143050       0       0  \n",
      "143051       0       0  \n",
      "143052       0       0  \n",
      "143053       0       0  \n",
      "143054       0       0  \n",
      "143055       0       0  \n",
      "143056       0       0  \n",
      "143057       0       0  \n",
      "143058       0       0  \n",
      "143059       0       0  \n",
      "143060       0       0  \n",
      "143061       0       0  \n",
      "\n",
      "[143062 rows x 1025 columns]\n",
      "0         1\n",
      "1         0\n",
      "2         0\n",
      "3         1\n",
      "4         0\n",
      "5         1\n",
      "6         1\n",
      "7         1\n",
      "8         1\n",
      "9         0\n",
      "10        0\n",
      "11        0\n",
      "12        0\n",
      "13        0\n",
      "14        1\n",
      "15        0\n",
      "16        0\n",
      "17        1\n",
      "18        0\n",
      "19        0\n",
      "20        0\n",
      "21        0\n",
      "22        1\n",
      "23        0\n",
      "24        1\n",
      "25        1\n",
      "26        1\n",
      "27        0\n",
      "28        0\n",
      "29        0\n",
      "         ..\n",
      "143032    0\n",
      "143033    0\n",
      "143034    0\n",
      "143035    0\n",
      "143036    1\n",
      "143037    0\n",
      "143038    0\n",
      "143039    0\n",
      "143040    1\n",
      "143041    0\n",
      "143042    0\n",
      "143043    1\n",
      "143044    0\n",
      "143045    1\n",
      "143046    1\n",
      "143047    1\n",
      "143048    0\n",
      "143049    1\n",
      "143050    1\n",
      "143051    0\n",
      "143052    0\n",
      "143053    1\n",
      "143054    0\n",
      "143055    0\n",
      "143056    0\n",
      "143057    0\n",
      "143058    1\n",
      "143059    0\n",
      "143060    1\n",
      "143061    1\n",
      "Name: target, Length: 143062, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataFrame_same_gsc = pd.read_csv('same_pairs_gsc.csv')\n",
    "#print(dataFrame_same)\n",
    "dataFrame_diff_gsc = pd.read_csv('diffn_pairs_gsc.csv')\n",
    "#print(dataFrame_diff)\n",
    "dataFrame_diff_random_gsc = dataFrame_diff_gsc.sample(n=len(dataFrame_same_gsc))\n",
    "#print(dataFrame_diff_random)\n",
    "same_diff_one_gsc = dataFrame_same_gsc.append(dataFrame_diff_random_gsc)\n",
    "#print(same_diff_one)\n",
    "same_diff_shuffle_gsc = shuffle(same_diff_one_gsc)\n",
    "#print(same_diff_shuffle) \n",
    "dataFrame_features_gsc = pd.read_csv('GSC-Features.csv')\n",
    "#print(dataFrame_human_features)\n",
    "\n",
    "get_features_gsc = pd.merge(same_diff_shuffle_gsc,dataFrame_features_gsc, how='left', left_on='img_id_A', right_on='img_id')\n",
    "#print(get_features_gsc)\n",
    "get_features_gsc.drop(columns = ['img_id_A', 'img_id'], axis=1,inplace=True)\n",
    "get_features_gsc=pd.merge(get_features_gsc,dataFrame_features_gsc,how='left',left_on='img_id_B',right_on='img_id')\n",
    "get_features_gsc.drop(columns = ['img_id_B', 'img_id'], axis=1,inplace=True)\n",
    "\n",
    "#features_sub = pd.DataFrame(columns=['f1_x', 'f2_x', 'f3_x', 'f4_x', 'f5_x', 'f6_x', 'f7_x', 'f8_x', 'f9_x'])\n",
    "\n",
    "features_final_gsc=get_features_gsc.loc[:,:]\n",
    "feature_target_gsc=get_features_gsc.loc[:,\"target\"]\n",
    "\n",
    "\n",
    "print(features_final_gsc)\n",
    "print(feature_target_gsc)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_final_gsc.drop(columns = ['target'], axis=1,inplace=True)\n",
    "features_final_gsc.to_csv('New_Features_gsc.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_target_gsc.to_csv('New_Target_gsc.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Different Parameters & Functions : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxAcc = 0.0 \n",
    "#initialize Max Accurary = 0.0 \n",
    "\n",
    "maxIter = 0 \n",
    "#initialize Max Iterations = 0\n",
    "\n",
    "C_Lambda = 3\n",
    "#initialize Lambda value\n",
    "\n",
    "TrainingPercent = 80 \n",
    "#initialize Training Data Set % = 80\n",
    "\n",
    "ValidationPercent = 10 \n",
    "#initialize Validation Data Set % = 10\n",
    "\n",
    "TestPercent = 10 \n",
    "#initialize Test Data Set % = 10 \n",
    "\n",
    "M = 10 \n",
    "#initialize number of M Basis Functions = 10\n",
    "\n",
    "PHI = [] \n",
    "#initialize PHI as an Array \n",
    "\n",
    "IsSynthetic = False \n",
    "#set IsSynthetic value as False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation for Logistic Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetTargetVector(filePath):                         #define GetTarget Vector code block with source filePath\n",
    "    t = []                                             #define target vector array\n",
    "    with open(filePath, 'rU') as f:                    #open or set directory for the source filePath as f\n",
    "        reader = csv.reader(f)                         #open filePath defined as f in csv reader \n",
    "        for row in reader:                             #from the filepath access rows \n",
    "            t.append(int(row[0]))                      #append row values to target array\n",
    "    return t                                           #return the values to target dataset t\n",
    "\n",
    "def GenerateRawData(filePath, IsSynthetic): \n",
    "                                 #define GenerateRawData with source filePath and IsSynthetic value set as False    \n",
    "    dataMatrix = []              #define dataMatrix as an array to store values\n",
    "    with open(filePath, 'rU') as fi: #open or set directory for the source filePath as fi\n",
    "        reader = csv.reader(fi)      #open filePath defined as fi in csv reader\n",
    "        for row in reader:           #from the filepath access rows\n",
    "            dataRow = []             #dataRow array saves the values\n",
    "            for column in row: #for every column in datarow\n",
    "                dataRow.append(float(column)) #append the value of dataRow as float\n",
    "            dataMatrix.append(dataRow)        #append the dataRow matrix in dataMatrix   \n",
    "    \n",
    "    if IsSynthetic == False : \n",
    "        dataMatrix = np.delete(dataMatrix, [5,6,7,8,9], axis=1) # delete array elements from dataMatrix \n",
    "    dataMatrix = np.transpose(dataMatrix)                       # take the transpose of the dataMatrix     \n",
    "    return dataMatrix                                  #return the dataMatrix which was transposed in above line\n",
    "\n",
    "def GenerateTrainingTarget(rawTraining,TrainingPercent = 80): \n",
    "                                                    #define GenerateTrainingTarget data with data and percentage\n",
    "    TrainingLen = int(math.ceil(len(rawTraining)*(TrainingPercent*0.01))) \n",
    "                                                   #covert to float and put the values in TrainingLen\n",
    "    t           = rawTraining[:TrainingLen]        #put the values of rawTraining set into target vector array t\n",
    "                                                 \n",
    "    return t                                       #return new target vector array t\n",
    "\n",
    "def GenerateTrainingDataMatrix(rawData, TrainingPercent = 80): \n",
    "                                                  #define GenerateTrainingDataMatrix with rawData and percentage\n",
    "    T_len = int(math.ceil(len(rawData[0])*0.01*TrainingPercent))\n",
    "                                                  #covert to float and put the values in T_Len\n",
    "    d2 = rawData[:,0:T_len]                       #put values in d2 \n",
    "    return d2                                     #return d2 with changes\n",
    "\n",
    "def GenerateValData(rawData, ValPercent, TrainingCount): \n",
    "                                              #define GenerateValData with rawData, TrainingCount and ValPercent \n",
    "    valSize = int(math.ceil(len(rawData[0])*ValPercent*0.01)) #calculate val_size with float values\n",
    "    V_End = TrainingCount + valSize \n",
    "    dataMatrix = rawData[:,TrainingCount+1:V_End]             #update the dataMatrix with new values \n",
    "    return dataMatrix                                         #return the updated dataMatrix\n",
    "\n",
    "def GenerateValTargetVector(rawData, ValPercent, TrainingCount): \n",
    "                                                               #define GenerateValTargetVector for taget vectors \n",
    "    valSize = int(math.ceil(len(rawData)*ValPercent*0.01))     #calculate val_size with float values\n",
    "    V_End = TrainingCount + valSize\n",
    "    t =rawData[TrainingCount+1:V_End]                          #update the target Vector values with new values\n",
    "    return t                                                   #return the updated array t\n",
    "\n",
    "def GenerateBigSigma(Data, MuMatrix,TrainingPercent,IsSynthetic): \n",
    "                                                               #define GenerateBigSigma for traning Vectors\n",
    "    BigSigma    = np.zeros((len(Data),len(Data)))              #BigSigma takes the values of the Data\n",
    "    DataT       = np.transpose(Data)                           #take the transpose of the Data Matrix \n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01)) #calculate Training Lenth with float values       \n",
    "    varVect     = [] \n",
    "    for i in range(0,len(DataT[0])):                           #iterate the values for tranpose data range\n",
    "        vct = []\n",
    "        for j in range(0,int(TrainingLen)):                    #iterate the values for TrainingLen range\n",
    "            vct.append(Data[i][j])                             #append the values in array vct \n",
    "        varVect.append(np.var(vct))                            #append the values in array varVect\n",
    "    \n",
    "    for j in range(len(Data)):                                 #iterate the values for data range\n",
    "        BigSigma[j][j] = varVect[j]\n",
    "    if IsSynthetic == True:                                    #if value True\n",
    "        BigSigma = np.dot(3,BigSigma)                          #calculate BigSigma\n",
    "    else: #if value False\n",
    "        BigSigma = np.dot(200,BigSigma)                        #calculate BigSigma\n",
    "        Add_Noise_BigSigma = BigSigma+0.00001*np.random.rand(1024,1024) #Add Noise to Big Sigma\n",
    "    return Add_Noise_BigSigma                                           #return the updated value of the BigSigma\n",
    "\n",
    "def GetScalar(DataRow,MuRow, BigSigInv):                       #define the GetScalar\n",
    "    R = np.subtract(DataRow,MuRow)                             #calculate R\n",
    "    T = np.dot(BigSigInv,np.transpose(R))                      #calculate T   \n",
    "    L = np.dot(R,T)                                            #calculate L\n",
    "    return L                                                   #return the updated value of L \n",
    "\n",
    "def GetRadialBasisOut(DataRow,MuRow, BigSigInv):               #define the RadialBasis function    \n",
    "    phi_x = math.exp(-0.5*GetScalar(DataRow,MuRow,BigSigInv)) \n",
    "    return phi_x                                               #return the update value of phi_x\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80): #define the Phi Matrix \n",
    "    DataT = np.transpose(Data)                                    #take the transpose of DataT matrix\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))    #calculate TraningLen with float values         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))              #take the required PHI values\n",
    "    BigSigInv = np.linalg.inv(BigSigma)                           #calculate the BigSigma inverse value\n",
    "    for  C in range(0,len(MuMatrix)):\n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv) #calculate the PHI values\n",
    "    return PHI                                                              #return updated PHI values\n",
    "\n",
    "def GetWeightsClosedForm(PHI, T, Lambda):                                    #define the GetWeightsCLosedForm\n",
    "    Lambda_I = np.identity(len(PHI[0]))                                      #Lamda_I = identity matrix of PHI\n",
    "    for i in range(0,len(PHI[0])):\n",
    "        Lambda_I[i][i] = Lambda\n",
    "    PHI_T       = np.transpose(PHI)                                          #calculate phi transpose\n",
    "    PHI_SQR     = np.dot(PHI_T,PHI)                                          #calculate phiT.phi\n",
    "    PHI_SQR_LI  = np.add(Lambda_I,PHI_SQR)                                   #calculate lambda + phiT.phi\n",
    "    PHI_SQR_INV = np.linalg.inv(PHI_SQR_LI)                                  #calculate phi inverse\n",
    "    INTER       = np.dot(PHI_SQR_INV, PHI_T)              #calculate dot product of phi inverse and transpose\n",
    "    W           = np.dot(INTER, T)                                           #calculate w\n",
    "    return W                                                                 #return the updated value of w\n",
    "\n",
    "def GetPhiMatrix(Data, MuMatrix, BigSigma, TrainingPercent = 80):  #define the GetPhiMatrix\n",
    "    DataT = np.transpose(Data)                                     #transpose of DataT\n",
    "    TrainingLen = math.ceil(len(DataT)*(TrainingPercent*0.01))     #calculate TraningLen with float values         \n",
    "    PHI = np.zeros((int(TrainingLen),len(MuMatrix)))               #Phi matrix  \n",
    "    BigSigInv = np.linalg.inv(BigSigma)                            #calculate the BigSigma Inverse\n",
    "    for  C in range(0,len(MuMatrix)): \n",
    "        for R in range(0,int(TrainingLen)):\n",
    "            PHI[R][C] = GetRadialBasisOut(DataT[R], MuMatrix[C], BigSigInv) \n",
    "                                                                      #for each range of Mu Matrix calculate phi\n",
    "    return PHI                                                        #return the updated values of Phi\n",
    "\n",
    "def GetValTest(VAL_PHI,W):                                            #define GetValTest for validation\n",
    "    Y = np.dot(W,np.transpose(VAL_PHI))                               #calculate W . ValPhi\n",
    "    return Y                                                          #return the update values of Y\n",
    "\n",
    "def GetErms(VAL_TEST_OUT,ValDataAct):                                 #define the GetERMS block\n",
    "    sum = 0.0                                                         #initialize sum = 0.0\n",
    "    t=0                                                               #initialize t = 0 for target vectors\n",
    "    accuracy = 0.0                                                    #initialize accuracy = 0.0\n",
    "    counter = 0                                                       #initialize counter to 0\n",
    "    val = 0.0                                                         #initialize val to 0\n",
    "    for i in range (0,len(VAL_TEST_OUT)): \n",
    "        sum = sum + math.pow((ValDataAct[i] - VAL_TEST_OUT[i]),2)     #calculate the sum\n",
    "        if(int(np.around(VAL_TEST_OUT[i], 0)) == ValDataAct[i]):\n",
    "            counter+=1                                                #increment the counter\n",
    "    accuracy = (float((counter*100))/float(len(VAL_TEST_OUT)))        #calculate the accuracy of the data\n",
    "    return (str(accuracy) + ',' +  str(math.sqrt(sum/len(VAL_TEST_OUT)))) # return the value of ERMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch and Prepare Dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\NEERAJ\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: 'U' mode is deprecated\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\NEERAJ\\AppData\\Local\\conda\\conda\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: 'U' mode is deprecated\n",
      "  if sys.path[0] == '':\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RawTarget = GetTargetVector('New_Target_gsc.csv')              #feed the machine the target ouput csv file\n",
    "RawData   = GenerateRawData('New_Features_gsc.csv',IsSynthetic)#feed the machine the concatenated output csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainingTarget = np.array(GenerateTrainingTarget(RawTarget,TrainingPercent)) #give traning target data\n",
    "TrainingData   = GenerateTrainingDataMatrix(RawData,TrainingPercent)         #give training data from matrix\n",
    "\n",
    "print(TrainingTarget.shape) \n",
    "print(TrainingData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Validation Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ValDataAct = np.array(GenerateValTargetVector(RawTarget,ValidationPercent, (len(TrainingTarget)))) \n",
    "ValData    = GenerateValData(RawData,ValidationPercent, (len(TrainingTarget))) \n",
    "#give the validation data with generated data\n",
    "\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Test Data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestDataAct = np.array(GenerateValTargetVector(RawTarget,TestPercent, (len(TrainingTarget)+len(ValDataAct))))\n",
    "TestData = GenerateValData(RawData,TestPercent, (len(TrainingTarget)+len(ValDataAct))) #give test data \n",
    "\n",
    "print(ValDataAct.shape)\n",
    "print(ValData.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closed Form Solution [Finding Weights using Moore- Penrose pseudo- Inverse Matrix ] :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ErmsArr = []                  \n",
    "#define the Erms array\n",
    "\n",
    "AccuracyArr = []              \n",
    "#define accuracy array\n",
    "\n",
    "kmeans = KMeans(n_clusters=M, random_state=0).fit(np.transpose(TrainingData)) \n",
    "#calculate the clustering using KMeans\n",
    "\n",
    "Mu = kmeans.cluster_centers_  \n",
    "#calculate the Mu value\n",
    "\n",
    "BigSigma     = GenerateBigSigma(RawData, Mu, TrainingPercent,IsSynthetic)   \n",
    "#calculate the BigSigma value\n",
    "\n",
    "TRAINING_PHI = GetPhiMatrix(RawData, Mu, BigSigma, TrainingPercent)         \n",
    "#calculate the traning phi value\n",
    "\n",
    "W            = GetWeightsClosedForm(TRAINING_PHI,TrainingTarget,(C_Lambda)) \n",
    "#calculate the regularizer value\n",
    "\n",
    "TEST_PHI     = GetPhiMatrix(TestData, Mu, BigSigma, 100) \n",
    "#get the Test phi value\n",
    "\n",
    "VAL_PHI      = GetPhiMatrix(ValData, Mu, BigSigma, 100) \n",
    "#get the val phi value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Mu.shape)\n",
    "print(BigSigma.shape)\n",
    "print(TRAINING_PHI.shape)\n",
    "print(W.shape)\n",
    "print(VAL_PHI.shape)\n",
    "print(TEST_PHI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Erms on Training, Validation and Test Set : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_TEST_OUT  = GetValTest(TRAINING_PHI,W) \n",
    "VAL_TEST_OUT = GetValTest(VAL_PHI,W)\n",
    "TEST_OUT     = GetValTest(TEST_PHI,W)\n",
    "\n",
    "TrainingAccuracy   = str(GetErms(TR_TEST_OUT,TrainingTarget)) #get erms of training data accuracy\n",
    "ValidationAccuracy = str(GetErms(VAL_TEST_OUT,ValDataAct))    #get erms of validation data accuracy\n",
    "TestAccuracy       = str(GetErms(TEST_OUT,TestDataAct))       #get erms of test data accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('UBITname      = nabhyank')\n",
    "print ('Person Number = 50290958')\n",
    "print ('----------------------------------------------------')\n",
    "print (\"------------------LeToR Data------------------------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"-------Closed Form with Radial Basis Function-------\")\n",
    "print ('----------------------------------------------------')\n",
    "print (\"M = \" + str (M))\n",
    "print (\"Lambda = \" + str(C_Lambda))\n",
    "print (\"E_rms Training   = \" + str(float(TrainingAccuracy.split(',')[1])))\n",
    "print (\"Traning Accuracy   = \" + str(float(TrainingAccuracy.split(',')[0])))\n",
    "print (\"E_rms Validation = \" + str(float(ValidationAccuracy.split(',')[1])))\n",
    "print (\"Validation Accuracy   = \" + str(float(ValidationAccuracy.split(',')[0])))\n",
    "print (\"E_rms Testing    = \" + str(float(TestAccuracy.split(',')[1])))\n",
    "print (\"Testing Accuracy   = \" + str(float(TestAccuracy.split(',')[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent solution for Logistic Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('----------------------------------------------------')\n",
    "print ('--------------Please Wait for 2 mins!----------------')\n",
    "print ('----------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Now        = np.dot(220, W)                                        #take the dot product to calculate W now \n",
    "La           = 4                                                     #initialize Lambda value\n",
    "learningRate = 0.03                                                  #set the learning rate value\n",
    "L_Erms_Val   = []                                                    #define Erms value of validation data array\n",
    "L_Erms_Acc_Val   = []                                                #define Erms value of accuracy data array\n",
    "L_Erms_TR    = []                                                    #define Erms value of training data array\n",
    "L_Erms_Acc_TR   = []                                                 #define Erms value of accuracy data array\n",
    "L_Erms_Test  = []                                                    #define Erms value of testing data array\n",
    "L_Erms_Acc_Test   = []                                               #define Erms value of accuracy data array\n",
    "W_Mat        = []                                                    #define W\n",
    "\n",
    "for i in range(0,400):                                               #iterate for range of 40 \n",
    "    \n",
    "    a = math.exp(-np.dot(np.transpose(W_Now),TRAINING_PHI[i]))\n",
    "    #print ('---------Iteration: ' + str(i) + '--------------')\n",
    "    Delta_E_D     = -np.dot((TrainingTarget[i] - (1/(1+a))),TRAINING_PHI[i])\n",
    "    La_Delta_E_W  = np.dot(La,a) \n",
    "    #calculate change in E_W dot product with lambda\n",
    "    Delta_E       = np.add(Delta_E_D,La_Delta_E_W) \n",
    "    #calculate change in E    \n",
    "    Delta_W       = -np.dot(learningRate,Delta_E) \n",
    "    #change in W is negative of dot product of learning rate and change in E  \n",
    "    W_T_Next      = W_Now + Delta_W \n",
    "    #next W = current w + change in W\n",
    "    W_Now         = W_T_Next \n",
    "    #update current W to next W where W is the weight\n",
    "    \n",
    "    #-----------------TrainingData Accuracy---------------------#\n",
    "    TR_TEST_OUT   = GetValTest(a,W_T_Next) \n",
    "    Erms_TR       = GetErms(TR_TEST_OUT,TrainingTarget) \n",
    "    #calculate the erms training accuracy for all weights and phi values\n",
    "    L_Erms_TR.append(float(Erms_TR.split(',')[1])) \n",
    "    #append the values acquired in above calculation\n",
    "    L_Erms_Acc_TR.append(float(Erms_TR.split(',')[0]))\n",
    "    \n",
    "    #-----------------ValidationData Accuracy---------------------#\n",
    "    VAL_TEST_OUT  = GetValTest(a,W_T_Next) \n",
    "    Erms_Val      = GetErms(VAL_TEST_OUT,ValDataAct) \n",
    "    #calculate the erms validation accuracy for all values of testing set and actual data values\n",
    "    L_Erms_Val.append(float(Erms_Val.split(',')[1])) \n",
    "    #append the values acquired in above calculation\n",
    "    L_Erms_Acc_Val.append(float(Erms_Val.split(',')[0]))\n",
    "    \n",
    "    #-----------------TestingData Accuracy---------------------#\n",
    "    TEST_OUT      = GetValTest(a,W_T_Next) \n",
    "    Erms_Test = GetErms(TEST_OUT,TestDataAct) \n",
    "    #calculate the erms testing accuracy \n",
    "    L_Erms_Test.append(float(Erms_Test.split(',')[1])) \n",
    "    #append the values from above calculation\n",
    "    L_Erms_Acc_Test.append(float(Erms_Test.split(',')[0])) \n",
    "    #append the values from above calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('----------Gradient Descent Solution--------------------')\n",
    "print (\"M = \"+ str (M))\n",
    "print (\"La = \"+ str(La))\n",
    "print (\"Learning Rate = \"+ str(learningRate))\n",
    "print (\"E_rms Training   = \" + str(np.around(min(L_Erms_TR),5)))\n",
    "print (\"Traning Accuracy   = \" + str(np.around(max(L_Erms_Acc_TR),5)))\n",
    "print (\"E_rms Validation = \" + str(np.around(min(L_Erms_Val),5)))\n",
    "print (\"Validation Accuracy   = \" + str(np.around(max(L_Erms_Acc_Val),5)))\n",
    "print (\"E_rms Testing    = \" + str(np.around(min(L_Erms_Test),5)))\n",
    "print (\"Testing Accuracy   = \" + str(np.around(max(L_Erms_Acc_TR),5)))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(L_Erms_TR)\n",
    "plt.title('Training ERMS')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(L_Erms_Acc_TR)\n",
    "plt.title('Training Accuracy')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.figure(2)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(L_Erms_Val)\n",
    "plt.title('Validation ERMS')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(L_Erms_Acc_Val)\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.figure(3)\n",
    "plt.subplot(2,2,1)\n",
    "plt.plot(L_Erms_Test)\n",
    "plt.title('Testing ERMS')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.plot(L_Erms_Acc_Test)\n",
    "plt.title('Testing Accuracy')\n",
    "plt.xlabel('Number of data points given:')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
